name: Descargar datos públicos diarios

on:
  schedule:
    - cron: '*/5 * * * *' # Cada 5 minutos
  workflow_dispatch: # Permite ejecución manual

permissions:
  contents: write

jobs:
  fetch-and-store:
    runs-on: ubuntu-latest
    steps:
      - name: Clonar el repositorio
        uses: actions/checkout@v4

      - name: Instalar dependencias
        run: sudo apt-get update && sudo apt-get install -y wget jq

      - name: Instalar dependencias Python
        run: pip install -r requirements.txt

      - name: Crear carpetas por fecha
        run: |
          FECHA=$(date +'%Y-%m-%d')
          mkdir -p data/cpi/$FECHA
          mkdir -p data/liquidez/$FECHA

      - id: fecha
        run: echo "fecha=$(date +'%Y-%m-%d')" >> $GITHUB_OUTPUT

      - name: Ejecutar scraping de liquidez y guardar CSV
        run: |
          FECHA=$(date +'%Y-%m-%d')
          python -c "import os; os.makedirs(f'data/liquidez/{os.environ[\'FECHA\']}', exist_ok=True); from src.liquidez_scraper import fetch_liquidez_table, save_to_csv; datos = fetch_liquidez_table(); save_to_csv(datos, f'data/liquidez/{os.environ[\'FECHA\']}/datos.csv')"
        env:
          FECHA: ${{ steps.fecha.outputs.fecha }}

      - name: Hacer commit si hay cambios
        env:
          GH_TOKEN: ${{ secrets.GH_PAT }}
        run: |
          git config --global user.name "github-actions"
          git config --global user.email "actions@github.com"
          git add data/
          git diff --cached --quiet || git commit -m "Datos actualizados $FECHA"
          git push https://github-actions:${GH_TOKEN}@github.com/miguelnavarrouni/Automatizacion-webscraping.git HEAD:main
